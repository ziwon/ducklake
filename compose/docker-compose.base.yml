x-airflow-env: &airflow_env
  COMPOSE_PROJECT_NAME: "ducklake"
  AIRFLOW__CORE__LOAD_EXAMPLES: "False"
  AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
  AIRFLOW__CORE__FERNET_KEY: "0aV4KnSnXmdkB9GSXMKN7D6wEAAgMEuL+EiZgNjh0/0="
  AIRFLOW__DOCKER__REPOSITORY: "registry:5000"
  AIRFLOW__WEBSERVER__SECRET_KEY: "53f8faf97a03185e24d360c15bbb4eb19848beedc6b2cfcb747db819e835a5e3"
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://${POSTGRES_AIRFLOW_USER}:${POSTGRES_AIRFLOW_PASSWORD}@postgres/${POSTGRES_AIRFLOW_DB}"
  DOCKER_REGISTRY_URL: "http://registry:5000"
  MINIO_ENDPOINT_URL: "http://minio:9000"
  AWS_ACCESS_KEY_ID: "${MINIO_ROOT_USER}"
  AWS_SECRET_ACCESS_KEY: "${MINIO_ROOT_PASSWORD}"
  SPARK_MASTER_URL: "spark://spark-master:7077"
  HIVE_METASTORE_URI: "thrift://hive-metastore:9083"
  NESSIE_URI: "http://nessie:19120/api/v1"

networks:
  data-net:
    driver: bridge 

volumes:
  minio-data:
  postgres-data:
  registry-data:    
  spark-history-data:
  airflow-logs:

services:
  # ────────────────────────── MinIO (S3) ────────────────────────────────────
  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: "${MINIO_ROOT_USER}"
      MINIO_ROOT_PASSWORD: "${MINIO_ROOT_PASSWORD}"
    volumes: [minio-data:/data]
    ports: ["9000:9000","9001:9001"]
    networks: ["data-net"]
    healthcheck:
      test: ["CMD","curl","-f","http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  minio-init:
    image: minio/mc:latest
    entrypoint: >
      /bin/sh -c "
      echo 'Waiting for MinIO...' &&
      until mc alias set local http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}; do sleep 2; done &&
      mc mb -p local/${MINIO_DEFAULT_BUCKET} &&
      mc policy set download local/${MINIO_DEFAULT_BUCKET} &&
      echo 'Bucket ${MINIO_DEFAULT_BUCKET} ready.' &&
      sleep infinity"
    depends_on:
      minio:
        condition: service_healthy
    networks: ["data-net"]


  # ─────────────────── Postgres for Airflow + Hive + Nessie Catalog ───────────────────
  postgres:
    image: postgres:16
    environment:
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      POSTGRES_DB: "${POSTGRES_DB}"
      POSTGRES_HOST_AUTH_METHOD: "trust"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ${PWD}/config/sql/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    networks: ["data-net"]
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ──────────────────────────── Spark Cluster ──────────────────────────────
  spark-master:
    image: bitnami/spark:3.5.5
    command: ["bash","-c","spark-class org.apache.spark.deploy.master.Master"]
    environment:
      - SPARK_MODE=master
    ports: ["7077:7077","8080:8080"]
    volumes:
      - spark-history-data:/opt/spark/history
    networks: ["data-net"]

  spark-worker-1:
    image: bitnami/spark:3.5.5
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=4G
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8081
    depends_on: [spark-master]
    ports: ["8081:8081"]
    networks: ["data-net"]

  spark-history:
    image: bitnami/spark:3.5.5
    command: ["bash","-c","spark-class org.apache.spark.deploy.history.HistoryServer"]
    environment:
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=/opt/bitnami/spark/history -Dspark.history.fs.cleaner.enabled=true -Dspark.history.fs.cleaner.interval=1d -Dspark.history.fs.cleaner.maxAge=7d
    volumes:
      - spark-history-data:/opt/bitnami/spark/history
    ports: ["18080:18080"]
    networks: ["data-net"]

  # ────────────────── Airflow DB migration & connection bootstrap ───────────
  airflow-init:
    build:
      context: ../docker/airflow
      dockerfile: Dockerfile
      args:
        DOCKER_GROUP_ID: "${DOCKER_GROUP_ID}"
    depends_on:
      postgres:
        condition: service_healthy
    environment: *airflow_env
    command:
      - bash
      - -ec
      - |
          env | sort &&
          airflow db migrate &&
          airflow users create \
            --username admin \
            --password admin \
            --role Admin \
            --firstname Admin \
            --lastname User \
            --email admin@example.com ||
          echo '[WARN] Admin user already exists, skipping...' &&
          # Add Spark connection
          airflow connections add spark_default \
            --conn-type spark \
            --conn-host spark-master \
            --conn-port 7077 ||
          echo '[WARN] spark_default connection already exists, skipping...' &&
          # Add MinIO S3 connection (using AWS type)
          airflow connections add minio_s3_conn \
            --conn-type aws \
            --conn-login ${MINIO_ROOT_USER} \
            --conn-password ${MINIO_ROOT_PASSWORD} \
            --conn-extra '{"endpoint_url": "http://minio:9000"}' || 
          echo '[WARN] minio_s3_conn connection already exists, skipping...' &&
          # Add Docker connection
          airflow connections add docker_default \
            --conn-type docker \
            --conn-host "registry:5000" \
            --conn-schema "var/run/docker.sock" \
            --conn-login "" \
            --conn-password "" \
            --conn-extra '{"reauth": false}' ||
          echo '[WARN] docker_default already exists, skipping...' &&
          echo '[SUCCESS] Airflow initialization completed!'
    group_add:
      - "${DOCKER_GROUP_ID}"
    networks: ["data-net"]

  airflow-web:
    build:
      context: ../docker/airflow
      dockerfile: Dockerfile
      args:
        DOCKER_GROUP_ID: "${DOCKER_GROUP_ID}"
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    command: webserver
    environment: *airflow_env
    ports: ["8088:8080"]
    volumes:
      - ${PWD}/src/dags:/opt/airflow/dags:ro
      - airflow-logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    group_add:
      - "${DOCKER_GROUP_ID}"
    healthcheck:
      test: ["CMD-SHELL","curl --fail http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks: ["data-net"]

  airflow-scheduler:
    build:
      context: ../docker/airflow
      dockerfile: Dockerfile
      args:
        DOCKER_GROUP_ID: "${DOCKER_GROUP_ID}"
    restart: unless-stopped
    depends_on:
      airflow-web:
        condition: service_healthy
    command: scheduler
    environment: *airflow_env
    volumes:
      - ${PWD}/src/dags:/opt/airflow/dags:ro
      - airflow-logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    group_add:
      - "${DOCKER_GROUP_ID}"
    networks: ["data-net"]

  # Triggerer is needed only for deferrable operators.  Keep it for safety.
  airflow-triggerer:
    build:
      context: ../docker/airflow
      dockerfile: Dockerfile
      args:
        DOCKER_GROUP_ID: "${DOCKER_GROUP_ID}"
    restart: unless-stopped
    depends_on:
      airflow-web:
        condition: service_healthy
    command: triggerer
    environment: *airflow_env
    volumes:
      - airflow-logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    group_add:
      - "${DOCKER_GROUP_ID}"
    networks: ["data-net"]

  registry:
    image: registry:2            
    container_name: ducklake-registry
    restart: unless-stopped
    networks: ["data-net"]           
    ports:
      - "5000:5000"                
    environment:
      REGISTRY_HTTP_ADDR: 0.0.0.0:5000
      REGISTRY_STORAGE_DELETE_ENABLED: "true"
    volumes:
      - registry-data:/var/lib/registry

  duckdb-ui:
    image: th3sven/duckdb-ui
    container_name: duckdb-ui
    ports:
      - "4213:4213"
    environment:
      - DUCKDB_DATABASE_FILENAME=/app/data/duckdb.db
    networks: [data-net]  